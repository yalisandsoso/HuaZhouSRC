{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36aaf14",
   "metadata": {},
   "source": [
    "# 一、Why neural networks work & why they often don't ?\n",
    "- Yann LeCun:   【2021】\n",
    "   This is another piece that basically says \"deep learning is not as impressive as you think because it's mere interpolation resulting from glorified curve fitting\".But in high dimension, there is no such thing as interpolation.\n",
    "   In high dimension, everything is extrapolation.\n",
    "  \n",
    "- You Don’t Understand Neural Networks Until You Understand the Universal Approximation Theorem\n",
    "   \n",
    "   为甚么需要争论这样的问题？\n",
    "   深度学习研究所依赖的两个概念：（为什么一些深度学习模型工作的好？）\n",
    "   1. 算法之所以工作得好，是因为它们正确内插训练数据\n",
    "   2. 在任务和数据集中只有内插，没有外推\n",
    "   \n",
    "   然而，LeCun 质疑了这两个常见观念，这将会导致泛化性危机！\n",
    "   \n",
    "  \n",
    "上面理解为泛化表现好的模型通过对已有的数据集数据进行了内插，并且认为在预测中出现的也只有内插，因此，结果表现好。\n",
    "\n",
    "但是，LeCun认为在高维度中，就只有外推了，所以不能够说明是因为更好的内插导致模型的泛化能力好。\n",
    "\n",
    "它们之间没有必然的关系。\n",
    "\n",
    "\n",
    "### 如果在高维上不能用内插/外推解释泛化性能，那么在高维度上表现良好的模型是什么导致的呢？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fa740",
   "metadata": {},
   "source": [
    "\n",
    "## 二、Why interpolation in high dimension not exist?\n",
    "- Interpolation & extrapolation\n",
    "\n",
    "  $$ y = 2*x+ b (0 < x < 10) $$\n",
    "  \n",
    "  If we plug value x=6 into our equation to predict, this is interpolation(Because our x value is among the range of values used to make the line best fit).Otherwise, it's extrapolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb2ed3",
   "metadata": {},
   "source": [
    "- 1989 年，George Cybenko 最早提出并证明了单一隐藏层、任意宽度、并使用 S 函数作为激励函数的前馈神经网络的通用近似定理。\n",
    "- 两年后 1991 年，Kurt Hornik 研究发现，激活函数的选择不是关键，前馈神经网络的多层神经层及多神经元架构才是使神经网络有成为通用逼近器的关键。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1a6e1",
   "metadata": {},
   "source": [
    "- 如果测试集数据在训练集数据之内，那么这个训练过程可以称之为内插的，否则，则是外推。\n",
    "<br>\n",
    "\n",
    "- 如果在训练过程是内插的，并且根据万能近似定理（深度网络能够任意逼近实数函数），那么这个过程可以看做是一个 做的好的曲线拟合过程。并且使用测试集得到的loss一定程度可以说明模型的泛化能力。（新的数据落在数据集的凸包内）\n",
    "<br>\n",
    "\n",
    "- 但是如果如LeCun而言，高维度的情况下，任意测试集数据都是落在训练集的凸包之外，也就是说，训练过程是外推，模型在测试集合上只是进行了外推判断，又外推是说明不了泛化性的，因此，模型在测试集上表现好，并不能够说明模型的泛化性好。\n",
    "<br>\n",
    "\n",
    "- 外推本身就具有不确定性。\n",
    "\n",
    "### 我惊了!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e75d88",
   "metadata": {},
   "source": [
    "## 三、Learning in High Diemension Always Amounts to Extrapolation\n",
    "\n",
    "- the behavior of a model within a training set's convex hull barely impacts that model's generalization performance since new samples lie almost surely outside of that convex hull.\n",
    "\n",
    "模型在训练集凸包内的表现几乎不会影响该模型的泛化能力，因为新样本几乎肯定在该凸包之外（外推）。\n",
    "\n",
    "https://www.youtube.com/watch?v=-GH9vW5FWUs <br>\n",
    "https://www.youtube.com/watch?v=86ib0sfdFtw\n",
    "\n",
    "- interpolation/extrapolation definition as an indicator of generalization performances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e31af",
   "metadata": {},
   "source": [
    "# 四、Conclusions\n",
    "1. 旧观念，某些模型的泛化能力好的原因是，测试集在训练集的凸包之内（内插），而模型在训练集上的表现好，于是在测试集上的泛化能力很好。\n",
    "\n",
    "<br>\n",
    "\n",
    "2. 新观念：LeCun证明在高维上，测试集一定在训练集的凸包之外（外推），此时模型在训练集上的表现好，在测试集上的泛化能力依然很好。\n",
    "\n",
    "<br>\n",
    "\n",
    "也就是说明，模型的泛化能力不是以外推、内插作标准的（不能够认为测试集在训练集的凸包之内，出现内插，因此泛化能力好），模型在数据集凸包上的表现，也不能够说明模型的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a36fe4",
   "metadata": {},
   "source": [
    "# 五、Crisis\n",
    "\n",
    "- 泛化性该如何定义呢？是什么决定了模型的泛化性能的？\n",
    "- 要增强模型的泛化性应该怎么做呢？\n",
    "- We shouldn’t use interpolation/extrapolation in the way the terms are defined below when talking about generalization, because for high dimensional data deep learning models always have to extrapolate, regardless of the dimension of the underlying data manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d0df8",
   "metadata": {},
   "source": [
    "That is, given the realistic amount\n",
    "of data that can be carried by current computational capacities, it is extremely unlikely that a newly\n",
    "observed sample lies in the convex hull of that dataset. Hence, we claim that\n",
    "- currently employed/deployed models are extrapolating\n",
    "- given the super-human performances achieved by those models, extrapolation regime is not necessarily to be avoided, and is not an indicator of generalization performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2f81b",
   "metadata": {},
   "source": [
    "1. 高维只有外推\n",
    "2. 有一个模型在训练集上进行了外推，也就是测试集属于外推，表现有不确定性\n",
    "3. 模型上线，泛化能力表现好\n",
    "\n",
    "--> 外推不是需要被避免的，是否外推也不能够作为泛化性好坏的实际指标。\n",
    "\n",
    "论文里面所指出的外推概念（新样本在模型上的表现）和泛化性的表述有些模糊不清的地方。\n",
    "\n",
    "\n",
    "论文的大概意思是：以前我们习惯性考虑用模型在测试集上的表现来作为模型泛化性好坏的指标。实质上，当模型维度>100时，测试集落在训练集的凸包内的概率趋向于零，也就是说，在高维度，只有外推，测试集在训练集上表现好，是在测试集上的外推表现好，不能够作为泛化性指标（在实际数据集上的外推表现）。至少，我们不再能够根据模型在测试集上准确率99.9%，就说这个模型泛化性好。我们需要一个新的指标去衡量泛化性好坏。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69bc2d",
   "metadata": {},
   "source": [
    "# 六、Future\n",
    "\n",
    "- 假设正如Yann LeCun所言，在高维的模型里面，只有外推，也就是测试集的数据点都是落在训练集的凸包外边的，那么事实上，我们在训练过程做的事情就没有那么简单可以解释了，因为最后我们在测试集上同样表现的很优秀。如果我们训练只是在内插数据（逼近函数）的话，我们更大概率看到的是在测试集上表现的混乱。但是，这并不会。\n",
    "\n",
    "- 问题在于深度学习模型为什么可以很好的外推？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70899a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gaopy3.7",
   "language": "python",
   "name": "gaopy3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
